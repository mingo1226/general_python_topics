#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Nov 29 09:50:00 2019

@author: Andrew
"""
#hold off on these two installs, try below first
#pip install google-cloud    
#pip install google-cloud-vision

#try this one first
#pip install --upgrade google-cloud-bigquery

from google.cloud import bigquery

#export GOOGLE_APPLICATION_CREDENTIALS='/Users/Andrew/Library/Mobile Documents/com~apple~CloudDocs/Grad School/sql-query-project-8d69934ea4cd.json'

def implicit():
    from google.cloud import storage

    # If you don't specify credentials when constructing the client, the
    # client library will look for credentials in the environment.
    storage_client = storage.Client()

    # Make an authenticated API request
    buckets = list(storage_client.list_buckets())
    print(buckets)

from google.oauth2 import service_account

credentials = service_account.Credentials.from_service_account_file(
    '/Users/Andrew/Library/Mobile Documents/com~apple~CloudDocs/Grad School/sql-query-project-8d69934ea4cd.json')

project_id = 'sql-query-project'

# Create a "Client" object
client = bigquery.Client(credentials= credentials,project=project_id)

# Construct a reference to the "stackoverflow" dataset
dataset_ref = client.dataset("stackoverflow", project="bigquery-public-data")

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

tables = list(client.list_tables(dataset))
list_of_tables = [table.table_id for table in tables]
# Print your answer
print(list_of_tables)

# Contruct a reference to the "tags" table
tags_table_ref = dataset_ref.table("tags")

# API request - fetch the table
tags_table = client.get_table(tags_table_ref)

#preview tags table
client.list_rows(tags_table, max_results = 10).to_dataframe()

# Construct a reference to the "posts_answers" table
answers_table_ref = dataset_ref.table("posts_answers")

# API request - fetch the table
answers_table = client.get_table(answers_table_ref)

# Preview the first five lines of the "posts_answers" table
client.list_rows(answers_table, max_results=5).to_dataframe()

# Construct a reference to the "posts_questions" table
questions_table_ref = dataset_ref.table("posts_questions")

# API request - fetch the table
questions_table = client.get_table(questions_table_ref)

# Preview the first five lines of the "posts_questions" table
client.list_rows(questions_table, max_results=5).to_dataframe()

questions_query = """
                  SELECT id, title, owner_user_id
                  FROM `bigquery-public-data.stackoverflow.posts_questions`
                  WHERE tags LIKE '%bigquery%'
                  """
                  
# Set up the query (cancel the query if it would use too much of 
# your quota, with the limit set to 1 GB)
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)
questions_query_job = client.query(questions_query, job_config= safe_config)

# API request - run the query, and return a pandas DataFrame
questions_results = questions_query_job.to_dataframe()

# Preview results
print(questions_results.head())

#practice query - search for user IDs who have highest scores
votes_table_ref = dataset_ref.table("votes")
votes_table = client.get_table(votes_table_ref)
votes_table.schema



votes_query =  """
           SELECT COUNT(id) as id_count, EXTRACT(MONTH FROM creation_date) AS month
           FROM `bigquery-public-data.stackoverflow.votes`
           WHERE EXTRACT(YEAR FROM creation_date) < 2014
           GROUP BY month
           ORDER BY id_count DESC
           """
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)
votes_query_job = client.query(votes_query, job_config= safe_config)
votes_results = votes_query_job.to_dataframe()

print(votes_results.head(12))

#practice query rank users by most up_votes

users_table_ref = dataset_ref.table("users")
users_table = client.get_table(users_table_ref)
users_table.schema

users_query =  """
           SELECT id, up_votes, down_votes, views
           FROM `bigquery-public-data.stackoverflow.users`
           WHERE views > 10000
           ORDER BY up_votes DESC
           """           
           
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)
users_query_job = client.query(users_query, job_config= safe_config)
users_results = users_query_job.to_dataframe()

print(users_results.head(12))           

#rank users by their reputation score
reputation_query =  """
           SELECT id, reputation
           FROM `bigquery-public-data.stackoverflow.users`
           WHERE views > 1000
           ORDER BY reputation DESC
           """
           
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)
reputation_query_job = client.query(reputation_query, job_config= safe_config)
reputation_results = reputation_query_job.to_dataframe()

print(reputation_results.head(12))               

def expert_finder(topic, client):
    '''
    Returns a DataFrame with the user IDs who have written Stack Overflow answers on a topic.

    Inputs:
        topic: A string with the topic of interest
        client: A Client object that specifies the connection to the Stack Overflow dataset

    Outputs:
        results: A DataFrame with columns for user_id and number_of_answers. Follows similar logic to bigquery_experts_results shown above.
    '''
    my_query = """
           SELECT a.owner_user_id AS user_id, COUNT(1) AS number_of_answers
           FROM `bigquery-public-data.stackoverflow.posts_questions` AS q
           INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a
               ON q.id = a.parent_Id
           WHERE q.tags like @topic_finder
           GROUP BY a.owner_user_id
           ORDER BY number_of_answers DESC
           """

    # Set up the query (a real service would have good error handling for 
    # queries that scan too much data)
    query_params = [bigquery.ScalarQueryParameter("topic_finder", "STRING", '%' + topic + '%')]
    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**11, query_parameters = query_params)      
    my_query_job = client.query(my_query, job_config=safe_config)

    # API request - run the query, and return a pandas DataFrame
    results = my_query_job.to_dataframe()

    return results.head()

expert_finder('tensorflow', client)


